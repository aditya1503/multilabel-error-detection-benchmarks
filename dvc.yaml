stages:
  make_dataset:
    cmd: python src/data/make_dataset.py
    deps:
    - src/data/make_dataset.py
    params:
    - dataset_kwargs
    - seeds
    outs:
    - data/generated
  train:
    cmd: python src/models/train.py
    deps:
    - data/generated/
    - src/models/train.py
    params:
    - train
    outs:
    - data/pred_probs/
  plot_avg_trace:
    cmd: python src/data/plot_avg_trace.py
    deps:
    - src/data/plot_avg_trace.py
    params:
    - dataset_kwargs.small.gamma
    plots:
    - data/images/avg_trace.svg
  get_avg_accuracy:
    cmd: python src/models/avg_accuracy.py
    deps:
    - data/generated
    - src/models/avg_accuracy.py
    outs:
    - data/accuracy/results.csv:
        persist: true
  group_stats:
    cmd: python src/models/group_stats.py
    deps:
    - data/accuracy/results.csv
    - src/models/group_stats.py
    metrics:
    - data/accuracy/results_group.csv:
        cache: false
    outs:
    - data/accuracy/results_group.tex:
        cache: false
  score_classes:
    cmd: python src/evaluation/class_score.py
    deps:
    - data/pred_probs/
    - src/evaluation/class_score.py
    outs:
    - data/scores/class_scores.pkl
  aggregate:
    cmd: python src/evaluation/score.py
    deps:
    - data/scores/class_scores.pkl
    - src/evaluation/aggregate.py
    - src/evaluation/score.py
    params:
    - eval
    outs:
    - data/scores/scores.pkl
  rank_metrics:
    desc: Evalute performance of aggregated label quality scores with label error
      detection and ranking metrics
    cmd: python src/evaluation/eval_ranking_metrics.py
    deps:
    - data/scores/scores.pkl
    - src/evaluation/eval_ranking_metrics.py
    outs:
    - data/scores/results.csv
  plot_metrics:
    cmd: python src/evaluation/plot_metrics.py
    deps:
    - data/scores/results.csv
    - src/evaluation/plot_metrics.py
    plots:
    - data/images/scores/
